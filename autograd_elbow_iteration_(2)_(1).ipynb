{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ghy9mJLJqbY1"
      },
      "source": [
        "## Mounting of Google Drive for use in  Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TDjyVNB_LdPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GKN-f4r40Bw4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e776788-dffb-40b6-965e-93d740779f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-03fdfedqo7o"
      },
      "source": [
        "## IMPORTATION OF REQUIRED LIBRARIES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPug11j-zmhn"
      },
      "outputs": [],
      "source": [
        "!pip install torch -q\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import random\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n",
        "from torch import Tensor,deg2rad,sin,asin,cos,sqrt#,min,max#\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_list=[os.path.join('/content/drive/MyDrive/networks',path) for path in os.listdir('/content/drive/MyDrive/networks')]"
      ],
      "metadata": {
        "id": "YKIwn1saxx_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dulYeyd5rP9b"
      },
      "source": [
        "## LOADING MY DATA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN-ZEcj5gfAk"
      },
      "source": [
        "## PLOTTING THE LOCATIONS OF THE MAP OF EUROPE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwDeRXM7rpsO"
      },
      "source": [
        "## DEFINING MY  DISTANCE FUNCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CI0ALy5cffxO"
      },
      "outputs": [],
      "source": [
        "# THIS FUNCTION CALCULATES THE HAVERSINE DISTANCE BETWEEN TWO POINTS\n",
        "def haversine(loc1, loc2):\n",
        "      R = 3959.87433 # this is in miles.  For Earth radius in kilometers use 6372.8 km\n",
        "      lat1=loc1[:,0]\n",
        "      lon1=loc1[:,1]\n",
        "      lat2=loc2[0]\n",
        "      lon2=loc2[1]\n",
        "      dLat = deg2rad(lat2 - lat1)# CONVERTING DEGREES TO RADIANS\n",
        "      dLon = deg2rad(lon2 - lon1)\n",
        "      lat1 = deg2rad(lat1)\n",
        "      lat2 = deg2rad(lat2)\n",
        "      a = sin(dLat/2)**2 + cos(lat1)*cos(lat2)*sin(dLon/2)**2\n",
        "      c = 2*asin(sqrt(a))\n",
        "      return R * c # RETURNS THE HAVERSINE DISTANCE"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k,l=1,1\n",
        "m,n=0.001,0.001\n",
        "p=0.001\n",
        "q=0.3\n",
        "z=0.3"
      ],
      "metadata": {
        "id": "EOPPCQIRXpNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocZXi__fsa1i"
      },
      "source": [
        "## DEFINING OUR OBJECTIVE FUNCTIONS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45eJycPDVqhF"
      },
      "outputs": [],
      "source": [
        "# THE FIRST OBJECTIVE FUNCTION HERE CALCULATES AND RETURNS THE MAXIMUM OF THE MINIMUM NODE TO CONTROLLER LATENCY\n",
        "def maxn2C_fn(city_locs, cont_locs):\n",
        "  dists = haversine(city_locs,cont_locs[0])\n",
        "  min_dists = dists\n",
        "  for i in range(1,cont_locs.shape[0]):\n",
        "    dists = haversine(city_locs,cont_locs[i])\n",
        "    min_dists = torch.min(min_dists, dists)\n",
        "  return min_dists.max()\n",
        "# THE SECOND OBJECTIVE FUNCTION HERE CALCULATES AND RETURNS THE MEAN OF THE MINIMUM NODE TO CONTROLLER LATENCY\n",
        "def mean2C_fn(city_locs, cont_locs):\n",
        "  dists = haversine(city_locs,cont_locs[0])\n",
        "  min_dists = dists\n",
        "  for i in range(1,cont_locs.shape[0]):\n",
        "    dists = haversine(city_locs,cont_locs[i])\n",
        "    min_dists = torch.min(min_dists, dists)\n",
        "    return min_dists.mean()\n",
        "# THE THIRD OBJECTIVE FUNCTION HERE CALCULATES AND RETURNS MAXIMUM INTERCONTROLLER LATENCY\n",
        "def maxC2C_fn(cont_locs,amb_num):\n",
        "  dists = haversine(cont_locs,amb_num[0])\n",
        "  max_dists = dists\n",
        "  for i in range(1,amb_num.shape[0]):\n",
        "    dists = haversine(cont_locs,amb_num[i])\n",
        "    max_dists = torch.max(max_dists, dists)\n",
        "  return max_dists.max()*0.1\n",
        "# THE FOURTH OBJECTIVE FUNCTION HERE CALCULATES AND RETURNS DIVISION OF THE SUM OF INTERCONTROLLER LATENCY BY 5C2\n",
        "def meanC2C_fn(cont_locs,amb_num):\n",
        "  dists = haversine(cont_locs,amb_num[0])\n",
        "  sum_dists = dists.sum()\n",
        "  for i in range(1,amb_num.shape[0]):\n",
        "    dists = haversine(cont_locs,amb_num[i]).sum()\n",
        "    sum_dists = sum_dists+ dists\n",
        "  return (sum_dists/20)*0.1\n",
        "def load_balancing(city_locs,cont_locs):\n",
        "   node_counts=np.zeros(cont_locs.shape[0])\n",
        "   for i in range(city_locs.shape[0]):\n",
        "    dists = haversine(cont_locs,city_locs[i])\n",
        "    node_counts[np.argmin(dists.clone().detach().numpy())]+=1\n",
        "   return np.ptp(node_counts)\n",
        "def max_tolerance(city_locs,cont_locs):\n",
        "  dists = haversine(city_locs,cont_locs[0])\n",
        "  max_dists = dists\n",
        "  for i in range(1,cont_locs.shape[0]):\n",
        "    dists = haversine(city_locs,cont_locs[i])\n",
        "    min_dists = torch.max(max_dists, dists)\n",
        "  return max_dists.max()\n",
        "def all_losses(city_locs,cont_locs):\n",
        "  l1=maxn2C_fn(city_locs,cont_locs).item()\n",
        "  l2=mean2C_fn(city_locs,cont_locs).item()\n",
        "  l3=maxC2C_fn(cont_locs,cont_locs).item()\n",
        "  l4=meanC2C_fn(cont_locs,cont_locs).item()\n",
        "  l5=max_tolerance(city_locs,cont_locs).item()\n",
        "  l6=load_balancing(city_locs,cont_locs)\n",
        "  return np.array([l1,l2,l3,l4,l5,l6])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def overall_loss(city_locs,cont_locs):\n",
        "  l1=maxn2C_fn(city_locs,cont_locs)\n",
        "  l2=mean2C_fn(city_locs,cont_locs)\n",
        "  l3=maxC2C_fn(cont_locs,cont_locs+0.0001)\n",
        "  l4=meanC2C_fn(cont_locs,cont_locs+0.0001)\n",
        "  l5=max_tolerance(city_locs,cont_locs)\n",
        "  return k*l1 + l*l2+m*l3+n*l4+p*l5\n",
        "def overall_metric( city_locs,cont_locs):\n",
        "  l1=maxn2C_fn(city_locs,cont_locs).item()\n",
        "  l2=mean2C_fn(city_locs,cont_locs).item()\n",
        "  l3=maxC2C_fn(cont_locs,cont_locs).item()\n",
        "  l4=meanC2C_fn(cont_locs,cont_locs).item()\n",
        "  l5=max_tolerance(city_locs,cont_locs).item()\n",
        "  l6=load_balancing(city_locs,cont_locs)\n",
        "  return (k*l1 + l*l2+m*l3+n*l4+p*l5)*(l6**q)\n",
        "def overall_cost(best_loss,no_of_cont):\n",
        "  return best_loss*(no_of_cont**z)"
      ],
      "metadata": {
        "id": "rp8HfYrvi-Hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftv5DXGNxo2h"
      },
      "source": [
        "## GENERATING OUR RANDOM INITIAL LOCATIONS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random,numpy\n",
        "from tqdm import tqdm\n",
        "from torch import manual_seed"
      ],
      "metadata": {
        "id": "o4gKUHOaiT1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cost_patience=3\n",
        "def find_optimal(pos_tensors):\n",
        "  lr=0.01\n",
        "  n_epochs = 200\n",
        "  early_stopping=5\n",
        "  main_seed=12\n",
        "  overalls=[]\n",
        "  opt_no_cont=3\n",
        "  iter=0\n",
        "  # dataloader to load our data in batches, here we are using a batch size of 1\n",
        "  for siz in range(3,min(pos_tensors.shape[0],20)):\n",
        "    iter+=1\n",
        "    manual_seed(42)\n",
        "    # using torch to generate a random of size 5 by 2\n",
        "    np.random.seed(42)\n",
        "    rand_cities =np.random.choice(range(pos_tensors.shape[0]),siz,replace=False).astype(int)\n",
        "    rand_cities=np.sort(rand_cities)\n",
        "    cont_locs=torch.tensor(pos_tensors.numpy()[rand_cities.astype(int)])# adding the mean of our longitudes and latitudes to enable faster convergence of the model\n",
        "    initial_locs = cont_locs.clone() # Save originals for later\n",
        "    cont_locs=cont_locs+0.0001 # adding little noise for calculation of gradients\n",
        "    cont_locs.requires_grad_(True)\n",
        "    val_losses = []\n",
        "    losses_sum=[]\n",
        "    verbose=4\n",
        "    final_locs=cont_locs.clone()\n",
        "    for epoch in range(n_epochs):\n",
        "    # Run through batches\n",
        "      loss =overall_loss(pos_tensors,cont_locs)\n",
        "      loss.backward() # Calc grads\n",
        "      cont_locs.data -= lr * cont_locs.grad.data # Update locs\n",
        "      cont_locs.grad = None # Reset gradients for next step\n",
        "      losses=overall_metric(pos_tensors,cont_locs)\n",
        "      if epoch>0 :\n",
        "        if losses< np.min(losses_sum):\n",
        "          final_locs=cont_locs.clone()\n",
        "          best_loss=val_loss\n",
        "        else :\n",
        "          best_loss=np.min(losses_sum)\n",
        "      losses_sum.append(losses)\n",
        "      val_loss=all_losses(pos_tensors,cont_locs)\n",
        "      #if epoch % verbose==0:\n",
        "         #print(\"After {0:.0f} epochs: maxN2c: {1:.2f} meanN2C: {2:.2f} maxICL: {3:.2f} meanICL: {4:.2f} Max Tolerance:{5:.2f} Load Balancing:{6:.2f} Overall: {7:.2f} \".format(epoch,val_loss[0],val_loss[1],val_loss[2],val_loss[3],val_loss[4],val_loss[5],losses))\n",
        "      if epoch > early_stopping:\n",
        "        if np.min(losses_sum[-early_stopping:])> np.min(losses_sum):\n",
        "        #  print ('Training ended by early stopping, best score at epoch {}'.format(epoch))\n",
        "          break\n",
        "    #print(\"Best scores at epoch {0} maxN2c: {1:.2f} meanN2C: {2:.2f} maxICL: {3:.2f} meanICL: {4:.2f} Overall: {5:.2f} \".format(epoch,best_loss[0],best_loss[1],best_loss[2],best_loss[3],best_loss.sum()))\n",
        "    if iter>1:\n",
        "      if overall_cost(best_loss,siz)<np.min(overalls):\n",
        "        opt_no_cont= siz\n",
        "    overalls.append(overall_cost(best_loss,siz))\n",
        "  return opt_no_cont"
      ],
      "metadata": {
        "id": "HlXGIYbPYIyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ABNR1jHzmhT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1686b0b-f5b7-4b64-ef72-8b79ff12d1bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3/3 [01:02<00:00, 20.86s/it]\n"
          ]
        }
      ],
      "source": [
        "fields=['lon','lat']\n",
        "lats=[]\n",
        "longs=[]\n",
        "opts=[]\n",
        "for path in tqdm(path_list[:-1]):\n",
        "  pos_data=pd.read_csv(path,usecols=fields) #reading from the raw Csv\n",
        "  pos_tensors=torch.Tensor(pos_data[['lat','lon']].values) #converting the longitudes and the latitudes to tensor format\n",
        "  lats.append(pos_data['lat'].values.tolist())\n",
        "  longs.append(pos_data['lon'].values.tolist())\n",
        "  opts.append(find_optimal(pos_tensors))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame({'lats':lats,'longs':longs})\n",
        "df['no_of_cont']=opts\n",
        "df.to_csv('cont_opt.csv',index=False)"
      ],
      "metadata": {
        "id": "QFLS_tF1GPXk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A_xo4Dp0exdn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}